# nano-opencode

[![CI](https://github.com/robotlearning123/nano-opencode/workflows/CI/badge.svg)](https://github.com/robotlearning123/nano-opencode/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> A minimal AI coding assistant for the terminal - inspired by OpenCode and Oh My OpenCode.

**Two flavors, one philosophy: simplicity over complexity.**

## Choose Your Version

### ğŸ Python Version: [python/](python/)
**387 lines to beat them all.**

Perfect for:
- Learning how AI agents work
- Quick prototyping
- Minimal dependencies (just litellm)
- Single-file simplicity

```bash
cd python/
pip install litellm
python nano.py "Add error handling to my code"
```

**Stats:**
- 387 lines of code
- 1 dependency (litellm)
- 6 tools
- 92.3% benchmark score (12/13 tasks)

[â†’ Python README](python/README.md)

---

### ğŸ“˜ TypeScript Version: [Root Directory]
**Production-ready with 90%+ features in 33x less code than OpenCode.**

Perfect for:
- Production use
- Session persistence
- Multi-model support (Claude + OpenAI)
- Type safety
- Comprehensive testing

```bash
npm install
npm run build
npm start
```

**Stats:**
- 1,558 lines of code
- 7 dependencies
- 7 tools
- 100% test coverage (35/35 tests passing)
- 33x smaller than OpenCode (~1,500 vs ~50,000 LOC)

---

## Why nano-opencode?

Research shows that agent performance comes from **model quality Ã— tool quality**, not framework complexity:

- **nano-agent** (300 LOC) achieves >70% on SWE-bench
- **OpenCode** (50,000+ LOC) achieves ~70% on SWE-bench
- **Claude Code** (complex framework) matches simpler agents

Both versions prove this: well-designed tools and clear prompts outperform massive frameworks.

## Features Comparison

| Feature | Python | TypeScript |
|---------|--------|------------|
| Multi-model support | âœ… (via litellm) | âœ… (native SDKs) |
| File operations | âœ… | âœ… |
| Shell commands | âœ… | âœ… |
| Code search | âœ… | âœ… |
| Session persistence | âŒ | âœ… (SQLite) |
| Streaming responses | âŒ | âœ… |
| Type safety | âŒ | âœ… |
| Unit tests | âŒ | âœ… (35 tests) |
| Interactive REPL | âœ… | âœ… |
| Benchmark proven | âœ… 92.3% | âœ… 100% tests |

## Quick Start

### Python Version
```bash
cd python/
pip install litellm
python nano.py
```

### TypeScript Version
```bash
npm install
npm run build
npm start
# or for development:
npm run dev
```

## Documentation

- **Python**: See [python/README.md](python/README.md)
- **TypeScript**: See sections below

## TypeScript Installation & Configuration

### Installation

```bash
# Install dependencies
npm install

# Build
npm run build

# Link globally (optional)
npm link
```

### Configuration

Set API keys in environment variables:

```bash
# For Claude (Anthropic)
export ANTHROPIC_API_KEY=your_key_here

# For OpenAI
export OPENAI_API_KEY=your_key_here
```

Or create a config file at `~/.config/nano-opencode/config.json`:

```json
{
  "provider": "anthropic",
  "model": "claude-sonnet-4-20250514",
  "maxTokens": 8192,
  "temperature": 0.7
}
```

## TypeScript Usage

### Interactive Mode

```bash
nano-opencode
# or
noc
```

Available commands:
- `/help` - Show help
- `/quit` - Exit
- `/clear` - Clear screen
- `/new` - Start new session
- `/sessions` - List recent sessions
- `/load <id>` - Load a session
- `/tools` - List available tools

### Single Prompt Mode

```bash
nano-opencode --prompt "Create a hello world program in Python"
```

### Options

- `-p, --provider <name>` - LLM provider (anthropic or openai)
- `-m, --model <name>` - Model to use
- `-s, --session <id>` - Resume a specific session
- `--prompt <text>` - Run a single prompt and exit

## Available Tools

Both versions include core tools:

- **read/read_file** - Read file contents with line numbers
- **write/write_file** - Write or overwrite files
- **edit/edit_file** - Replace specific strings in files
- **shell/bash** - Execute shell commands
- **find/glob** - List files with glob patterns
- **grep** - Search file contents with regex
- **list** (TypeScript only) - Directory listing

## Architecture

### Python
- Single file: `python/nano.py` (387 lines)
- Simple loop: LLM â†’ Tool Calls â†’ Results â†’ Repeat
- Zero abstractions

### TypeScript
```
src/
â”œâ”€â”€ app.ts              - Entry point
â”œâ”€â”€ cli.ts              - Interactive interface
â”œâ”€â”€ config.ts           - Configuration
â”œâ”€â”€ store.ts            - Session persistence
â”œâ”€â”€ types.ts            - Type definitions
â”œâ”€â”€ providers/          - AI integrations
â”‚   â”œâ”€â”€ anthropic.ts
â”‚   â”œâ”€â”€ openai.ts
â”‚   â””â”€â”€ index.ts
â””â”€â”€ tools/              - Tool implementations
    â”œâ”€â”€ read.ts
    â”œâ”€â”€ writefile.ts
    â”œâ”€â”€ edit.ts
    â”œâ”€â”€ bash.ts
    â”œâ”€â”€ glob.ts
    â”œâ”€â”€ grep.ts
    â”œâ”€â”€ list.ts
    â””â”€â”€ index.ts
```

## Testing

### Python
```bash
cd python/
python swe_bench_mini.py   # 5 basic tasks
python swe_bench_hard.py   # 5 harder tasks
```

### TypeScript
```bash
npm test                    # Run all tests
npm run typecheck          # Type checking only
npm run build              # Build only
```

## Benchmarks

### Python Version
- Basic tests: 3/3 (100%)
- Mini SWE-bench: 5/5 (100%)
- Hard SWE-bench: 4/5 (80%)
- **Total: 12/13 (92.3%)**

### TypeScript Version
- Unit tests: 35/35 (100%)
- Build: âœ… 0 errors
- Type check: âœ… strict mode

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Documentation

- [CHANGELOG.md](CHANGELOG.md) - Version history
- [METRICS.md](METRICS.md) - Performance metrics
- [DELIVERY-SUMMARY.md](DELIVERY-SUMMARY.md) - Project summary
- [CODE-REVIEW.md](CODE-REVIEW.md) - Comprehensive code review
- [PUBLISHING.md](PUBLISHING.md) - Publishing guide

## License

MIT - See [LICENSE](LICENSE)

---

*Built to prove a point: **complexity is the enemy of capability**.*
